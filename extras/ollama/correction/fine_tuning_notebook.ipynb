{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GrI8mqVS3tI"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.20.3 trl peft accelerate bitsandbytes unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xBFxJeCEiNJ"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, AutoTokenizer\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def interactive_test(model, tokenizer):\n",
        "    \"\"\"\n",
        "    Creates an interactive loop to test the model with user prompts.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Interactive Model Test ---\")\n",
        "    print(\"Enter a prompt to test the fine-tuned model.\")\n",
        "    print(\"Type 'save' to finish testing and save the GGUF model.\")\n",
        "    print(\"Type 'cancel' to exit without saving.\")\n",
        "    print(\"------------------------------------\")\n",
        "\n",
        "    while True:\n",
        "        # Get input from the user\n",
        "        user_input = input(\"\\nPrompt: \")\n",
        "\n",
        "        # Check for control commands\n",
        "        if user_input.lower() == \"save\":\n",
        "            print(\"\\nProceeding to save the model...\")\n",
        "            return True  # Signal to continue and save\n",
        "        elif user_input.lower() == \"cancel\":\n",
        "            print(\"\\nCanceling save. Exiting script.\")\n",
        "            return False  # Signal to exit without saving\n",
        "\n",
        "        # Prepare the input for the model\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        # Generate a response\n",
        "        outputs = model.generate(input_ids=inputs, max_new_tokens=256, use_cache=True)\n",
        "        response_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "        # Clean up the output to only show the assistant's part\n",
        "        # Note: The split logic may vary slightly based on the exact model output format.\n",
        "        # This is a common way to parse it.\n",
        "        assistant_response = response_text.split(\"<|assistant|>\")\n",
        "        if len(assistant_response) > 1:\n",
        "            clean_response = assistant_response[1].strip()\n",
        "        else:\n",
        "            # Fallback for models that don't add the user prompt to the output\n",
        "            # or have a different format.\n",
        "            if \"user\" in response_text and \"assistant\" in response_text:\n",
        "                 clean_response = response_text.split(\"assistant\")[-1].strip()\n",
        "            else:\n",
        "                 clean_response = response_text\n",
        "\n",
        "        print(f\"Model: {clean_response}\")\n",
        "\n",
        "# Import Dataset\n",
        "try:\n",
        "    with open(\"data.json\", \"r\") as f:\n",
        "        file = json.load(f)\n",
        "    print(\"Successfully loaded data.json.\")\n",
        "    print(\"Sample record:\", file[1])\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: data.json not found. Please upload it to the Colab session.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Load the base model\n",
        "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto detection\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Create the dataset directly from the loaded file\n",
        "dataset = Dataset.from_list(file)\n",
        "\n",
        "# Formatting prompts so they can be sent like this {\"context\": \"今天天气不错，不冷不热。晚饭后，我们决定去公园散步。那里的风景很优美，空气也很新鲜。\", \"target_sentence\": \"真是个锻练身体的好地方。\"}\n",
        "def format_prompts(batch):\n",
        "    \"\"\"\n",
        "    Takes a batch of examples and returns a list of formatted strings\n",
        "    in a chat format where the user input is a JSON object.\n",
        "    \"\"\"\n",
        "    contexts = batch[\"context\"]\n",
        "    target_sentences = batch[\"target_sentence\"]\n",
        "    outputs = batch[\"output\"]\n",
        "\n",
        "    texts = []\n",
        "    for context, target, output in zip(contexts, target_sentences, outputs):\n",
        "        input_json = {\n",
        "            \"context\": context,\n",
        "            \"target_sentence\": target\n",
        "        }\n",
        "\n",
        "        # The user provides the simple JSON, and the assistant provides the correction JSON followed by the EOS token.\n",
        "        prompt = f\"<|user|>\\n{json.dumps(input_json, ensure_ascii=False)}\\n<|assistant|>\\n{json.dumps(output, ensure_ascii=False)}<|endoftext|>\"\n",
        "        texts.append(prompt)\n",
        "\n",
        "    return texts\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=128,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    formatting_func=format_prompts,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=25,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        dataloader_pin_memory=False, # Important for Colab\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"--- Model Training Finished ---\")\n",
        "\n",
        "\n",
        "# # Merge and save the 16-bit model\n",
        "# merged_model_path = \"merged_16bit_model\"\n",
        "# model.save_pretrained_merged(merged_model_path, tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "# # Reload the merged model for testing\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name=merged_model_path,\n",
        "#     dtype=dtype,\n",
        "#     load_in_4bit=False,\n",
        "# )\n",
        "\n",
        "# Run interactive test\n",
        "should_save = interactive_test(model, tokenizer)\n",
        "\n",
        "# Save the final GGUF model if requested\n",
        "if should_save:\n",
        "    model.save_pretrained_gguf(\n",
        "        \"gguf_model\", tokenizer, quantization_method=\"q4_k_m\"\n",
        "    )\n",
        "    print(\"\\nGGUF model saved successfully in the 'gguf_model' file.\")\n",
        "    print(\"You can download it from the file browser on the left.\")\n",
        "else:\n",
        "    print(\"\\nExiting. The final GGUF model was not saved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}